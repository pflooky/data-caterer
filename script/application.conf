flags {
    enableCount = true
    enableCount = ${?ENABLE_COUNT}
    enableGenerateData = true
    enableGenerateData = ${?ENABLE_GENERATE_DATA}
    enableRecordTracking = true
    enableRecordTracking = ${?ENABLE_RECORD_TRACKING}
    enableDeleteGeneratedRecords = false
    enableDeleteGeneratedRecords = ${?ENABLE_DELETE_GENERATED_RECORDS}
}

folders {
    generatedPlanAndTaskFolderPath = "/opt/app/data-caterer"
    generatedPlanAndTaskFolderPath = ${?GENERATED_PLAN_AND_TASK_FOLDER_PATH}
    planFilePath = "/opt/app/plan/simple-json-plan.yaml"
    planFilePath = ${?PLAN_FILE_PATH}
    taskFolderPath = "/opt/app/task"
    taskFolderPath = ${?TASK_FOLDER_PATH}
    recordTrackingFolderPath = "/opt/app/data-caterer/recordTracking"
    recordTrackingFolderPath = ${?RECORD_TRACKING_FOLDER_PATH}
}

metadata {
    numRecordsFromDataSource = 10000
    numRecordsForAnalysis = 10000
    oneOfDistinctCountVsCountThreshold = 0.1
}

spark {
    master = "local[*]"
    master = ${?SPARK_MASTER}
    config {
        "spark.sql.cbo.enabled" = "true"
        "spark.sql.adaptive.enabled" = "true"
        "spark.sql.cbo.planStats.enabled" = "true"
        "spark.sql.legacy.allowUntypedScalaUDF" = "true"
        "spark.sql.statistics.histogram.enabled" = "true"
        "spark.sql.catalog.postgres" = ""
        "spark.sql.catalog.cassandra" = "com.datastax.spark.connector.datasource.CassandraCatalog"
        "spark.hadoop.fs.s3a.directory.marker.retention" = "keep"
        "spark.hadoop.fs.s3a.bucket.all.committer.magic.enabled" = "true"
    }
}

json {
    json {
    }
}

csv {
    csv {
    }
}

parquet {
    parquet {
    }
}

jdbc {
    postgresCustomer {
        url = "jdbc:postgresql://localhost:5432/customer"
        url = ${?POSTGRES_URL}
        user = "postgres"
        user = ${?POSTGRES_USER}
        password = "postgres"
        password = ${?POSTGRES_PASSWORD}
        driver = "org.postgresql.Driver"
    }
    mysql {
        url = "jdbc:mysql://localhost:3306/customer"
        url = ${?MYSQL_URL}
        user = "root"
        user = ${?MYSQL_USERNAME}
        password = "root"
        password = ${?MYSQL_PASSWORD}
        driver = "com.mysql.cj.jdbc.Driver"
    }
}


org.apache.spark.sql.cassandra {
    cassandra {
        spark.cassandra.connection.host = "localhost"
        spark.cassandra.connection.host = ${?CASSANDRA_HOST}
        spark.cassandra.connection.port = "9042"
        spark.cassandra.connection.port = ${?CASSANDRA_PORT}
        spark.cassandra.auth.username = "cassandra"
        spark.cassandra.auth.username = ${?CASSANDRA_USERNAME}
        spark.cassandra.auth.password = "cassandra"
        spark.cassandra.auth.password = ${?CASSANDRA_PASSWORD}
    }
}

http {
    httpbin {
        url = "http://localhost:80/put"
        url = ${?HTTP_URL}
    }
}

jms {
    solace {
        initialContextFactory = "com.solacesystems.jndi.SolJNDIInitialContextFactory"
        initialContextFactory = ${?SOLACE_INITIAL_CONTEXT_FACTORY}
        connectionFactory = "/jms/cf/default"
        connectionFactory = ${?SOLACE_CONNECTION_FACTORY}
        url = "smf://localhost:55554"
        url = ${?SOLACE_URL}
        user = "admin"
        user = ${?SOLACE_USER}
        password = "admin"
        password = ${?SOLACE_PASSWORD}
        vpnName = "default"
        vpnName = ${?SOLACE_VPN}
    }
}

kafka {
    kafkaAccount {
        kafka.bootstrap.servers = "localhost:9092"
        kafka.bootstrap.servers = ${?KAFKA_BOOTSTRAP_SERVERS}
    }
}

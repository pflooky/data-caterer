package com.github.pflooky.datagen.core.generator

import com.github.pflooky.datacaterer.api.model.{Count, Field, Generator, PerColumnCount, Schema, Step}
import com.github.pflooky.datagen.core.util.{Account, SparkSuite}
import net.datafaker.Faker
import org.apache.spark.sql.types.{DoubleType, IntegerType, StringType}
import org.apache.spark.sql.{Dataset, Encoder, Encoders}
import org.junit.runner.RunWith
import org.scalatestplus.junit.JUnitRunner

@RunWith(classOf[JUnitRunner])
class DataGeneratorFactoryTest extends SparkSuite {

  private val dataGeneratorFactory = new DataGeneratorFactory(new Faker() with Serializable)
  private val schema = Schema(Some(
    List(
      Field("id"),
      Field("amount", Some("double")),
      Field("debit_credit", Some("string"), Some(Generator("oneOf", Map("oneOf" -> List("D", "C"))))),
      Field("name", Some("string"), Some(Generator("regex", Map("regex" -> "[A-Z][a-z]{2,6} [A-Z][a-z]{2,8}")))),
      Field("code", Some("int"), Some(Generator("sql", Map("sql" -> "CASE WHEN debit_credit == 'D' THEN 1 ELSE 0 END")))),
    )
  ))
  private val simpleSchema = Schema(Some(List(Field("id"))))

  test("Can generate data for basic step") {
    val step = Step("transaction", "parquet", Count(total = Some(10)), Map("path" -> "sample/output/parquet/transactions"), schema)

    val df = dataGeneratorFactory.generateDataForStep(step, "parquet")
    df.cache()

    assert(df.count() == 10L)
    assert(df.columns sameElements Array("id", "amount", "debit_credit", "name", "code"))
    assert(df.schema.fields.map(x => (x.name, x.dataType)) sameElements Array(
      ("id", StringType),
      ("amount", DoubleType),
      ("debit_credit", StringType),
      ("name", StringType),
      ("code", IntegerType),
    ))
    val sampleRow = df.head()
    assert(sampleRow.getString(0).nonEmpty && sampleRow.getString(0).length <= 20)
    assert(sampleRow.getDouble(1) >= 0.0)
    val debitCredit = sampleRow.getString(2)
    assert(debitCredit == "D" || debitCredit == "C")
    assert(sampleRow.getString(3).matches("[A-Z][a-z]{2,6} [A-Z][a-z]{2,8}"))
    if (debitCredit == "D") assert(sampleRow.getInt(4) == 1) else assert(sampleRow.getInt(4) == 0)
  }

  test("Can generate data when number of rows per column is defined") {
    val step = Step("transaction", "parquet",
      Count(total = Some(10), perColumn = Some(PerColumnCount(List("id"), Some(2)))),
      Map("path" -> "sample/output/parquet/transactions"), simpleSchema)

    val df = dataGeneratorFactory.generateDataForStep(step, "parquet")
    df.cache()

    assert(df.count() == 20L)
    val sampleId = df.head().getAs[String]("id")
    val sampleRows = df.filter(_.getAs[String]("id") == sampleId)
    assert(sampleRows.count() == 2L)
  }

  test("Can generate data with generated number of rows per column by a generator") {
    val step = Step("transaction", "parquet", Count(Some(10),
      perColumn = Some(PerColumnCount(List("id"), None, Some(Generator("random", Map("min" -> "1", "max" -> "2"))))), None),
      Map("path" -> "sample/output/parquet/transactions"), simpleSchema)

    val df = dataGeneratorFactory.generateDataForStep(step, "parquet")
    df.cache()

    assert(df.count() >= 10L)
    assert(df.count() <= 20L)
    val sampleId = df.head().getAs[String]("id")
    val sampleRows = df.filter(_.getAs[String]("id") == sampleId)
    assert(sampleRows.count() >= 1L)
    assert(sampleRows.count() <= 2L)
  }

  test("Can generate data with generated number of rows generated by a data generator") {
    val step = Step("transaction", "parquet", Count(None,
      perColumn = None,
      generator = Some(Generator("random", Map("min" -> "10", "max" -> "20")))),
      Map("path" -> "sample/output/parquet/transactions"), simpleSchema)

    val df = dataGeneratorFactory.generateDataForStep(step, "parquet")
    df.cache()

    assert(df.count() >= 10L)
    assert(df.count() <= 20L)
    val sampleId = df.head().getAs[String]("id")
    val sampleRows = df.filter(_.getAs[String]("id") == sampleId)
    assert(sampleRows.count() == 1L)
  }

  ignore("Can run spark streaming output at 2 records per second") {
    implicit val encoder: Encoder[Account] = Encoders.kryo[Account]
    val df = sparkSession.readStream
      .format("rate").option("rowsPerSecond", "10").load()
      .map(_ => Account())
      .limit(100)
    val stream = df.writeStream
      .foreachBatch((batch: Dataset[_], id: Long) => println(s"batch-id=$id, size=${batch.count()}"))
      .start()
    stream.awaitTermination(11000)
  }
}
